{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dc04cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias a utilizar\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import imutils\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential #Se utiliza para crear modelos de redes neuronales secuenciales (Es aquel en el que las capas se apilan una encima de la otra)\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam #Se utiliza para ajustar los pesos y los sesgos de las redes neuronales durante el entrenamiento\n",
    "from keras.preprocessing.image import ImageDataGenerator #Permite generar imagenes aumentadas en tiempo real durante el entrenamiento, ayuda a aumentar el tamaño y la diversidad del conjunto de datos lo que mejora la capacidad de generalización del modelo\n",
    "\"\"\"\n",
    "Capas de las redes neuronales:\n",
    "Conv2D - Se realiza la convulsión en imagenes de entrada para extraer características/patrones importantes\n",
    "MaxPooling2D - Reduce la dimensionalidad y extrae características dominantes\n",
    "Dense - Es una capa completamente conectada, cada entrada se conecta con todos los nodos de salida\n",
    "Dropout - Esta capa se utiliza para regularizar el modelo y evitar el sobreajuste, apaga neuronas aleatoriamente durante el entrenamiento para evitar la dependencia excesiva\n",
    "Flatten - Aplana los datos multidimensionales en una matriz unidimensional antes de pasarlo a las capas completamente conectadas\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e076d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def almacenarRostro(personName):\n",
    "    time.sleep(3)\n",
    "    dataPath_mp4 = 'C:/Users/iamra/FacialRecognition/Videos' #Cambia a la ruta donde hayas almacenado Data\n",
    "    videoPath = dataPath_mp4 + '/' + personName + '.mp4' #Ruta completa\n",
    "\n",
    "    if not os.path.exists(videoPath): # Si no existe el video con ese nombre lo crea\n",
    "        captura = cv2.VideoCapture(0) # Definioms que usaremos la camara\n",
    "        salida = cv2.VideoWriter(videoPath,cv2.VideoWriter_fourcc(*'XVID'),20.0,(640,480)) # Cambiar con la otra camara\n",
    "        while True:\n",
    "            ret, imagen = captura.read() # Comienza a grabar\n",
    "            imagen = cv2.flip(imagen, 1) # Aplicamos efecto espejo\n",
    "            if ret == True: # Si detecta video\n",
    "                cv2.imshow('video', imagen) # Mostramos la imagen\n",
    "                salida.write(imagen) # Comenzamos a guardar la imagen\n",
    "                if cv2.waitKey(1) & 0xFF == ord('s'): # Si se presiona la tecla s, paramos de grabar y se guarda el video\n",
    "                      break\n",
    "            else: break\n",
    "        captura.release()\n",
    "        salida.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    else: \n",
    "        print(\"Ya existe un video con ese nombre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7c4c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def almacenarDatos(personName, cargo, edad):\n",
    "    dataPath_txt = 'C:/Users/iamra/FacialRecognition/Datos' # Cambiar en la otra computadora\n",
    "    filePath = dataPath_txt + '/' + personName + '.txt'\n",
    "\n",
    "    if not os.path.exists(filePath):\n",
    "        with open(filePath, \"w\") as file:\n",
    "            file.write(\"Nombre: \" + personName + os.linesep)\n",
    "            file.write(\"Cargo: \" + cargo +  os.linesep) \n",
    "            file.write(\"Edad: \" + edad)\n",
    "\n",
    "    else:\n",
    "        print(\"Ya existe un archivo con estos datos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96255957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generarInformes():\n",
    "    dataPath_txt = 'C:/Users/iamra/FacialRecognition/Datos'\n",
    "    Datos = []\n",
    "    Nombres = []\n",
    "    Cargos = []\n",
    "    Edades = []\n",
    "\n",
    "    peopleList = os.listdir(dataPath_txt)\n",
    "    for person in peopleList: \n",
    "        filePathPerson = dataPath_txt + '/' + person\n",
    "        with open(filePathPerson, \"r\") as file:\n",
    "            Datos.append(file.readlines())\n",
    "\n",
    "\n",
    "    for Dato in Datos:\n",
    "        aux = Dato[0].split()\n",
    "        Nombres.append(aux[1])\n",
    "        aux = Dato[2].split()\n",
    "        Cargos.append(aux[1])\n",
    "        aux = Dato[4].split()\n",
    "        Edades.append(aux[1])\n",
    "\n",
    "    print(Nombres)\n",
    "    print(Cargos)\n",
    "    print(Edades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbc2b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraerRostro(personName):\n",
    "    dataPath = 'C:/Users/iamra/FacialRecognition/Rostros'# Lugar donde se van a almacenar los rostros\n",
    "    personPath = dataPath + '/' + personName \n",
    "\n",
    "    if not os.path.exists(personPath): # Verifica si existe la carpete de la persona si no crea una\n",
    "        print('Carpeta creada: ',personPath)\n",
    "        os.makedirs(personPath)\n",
    "\n",
    "    mp_face_detection = mp.solutions.face_detection # Es el detector de rostros\n",
    "    mp_drawing = mp.solutions.drawing_utils # Esto es para colocar el recuadro y los puntos\n",
    "    cap = cv2.VideoCapture('C:/Users/iamra/FacialRecognition/Videos/' + personName + '.mp4')\n",
    "\n",
    "    count = 0 # Esta variable va a llevar la cuenta de las fotos que se han tomado\n",
    "\n",
    "    with mp_face_detection.FaceDetection(  # Gestor de contexto\n",
    "        min_detection_confidence=0.6) as face_detection:\n",
    "        while True:\n",
    "            ret, frame = cap.read() # Comenzamos a leer el video\n",
    "            if ret == False: # Verificamos si se esta leyendo de forma correcta\n",
    "                break\n",
    "            frame = imutils.resize(frame, width=640)\n",
    "            height, width, _ = frame.shape # Obtenemos las dimensipones de la imagen\n",
    "            aux_frame = frame.copy() # Copiamos el frame \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # cv2 lo lee como BGR y lo convertimos a RGB       \n",
    "            results = face_detection.process(frame_rgb) # Obtenemos las cordenadas de la cara y de los puntos de interes\n",
    "            k = cv2.waitKey(1) & 0xFF\n",
    "            if results.detections is not None: # Si detecta un rostro entra aqui \n",
    "                for detection in results.detections: # Se usa un for en caso de detectar mas de un rostro\n",
    "                    mp_drawing.draw_detection(frame, detection,\n",
    "                        mp_drawing.DrawingSpec(color=(0, 255, 255), circle_radius=2),\n",
    "                        mp_drawing.DrawingSpec(color=(255, 0, 255))) # Dibuja en un recuadro un rostro y con circulos los puntos de interes\n",
    "\n",
    "                    # Obtenemos la posicion de la cara\n",
    "                    x = int(detection.location_data.relative_bounding_box.xmin * width)\n",
    "                    y = int(detection.location_data.relative_bounding_box.ymin * height)\n",
    "                    w = int(detection.location_data.relative_bounding_box.width * width)\n",
    "                    h = int(detection.location_data.relative_bounding_box.height * height)\n",
    "\n",
    "                    # Recortamos el rostro y reajustamos el tamaño\n",
    "                    rostro = aux_frame[y:y+h,x:x+w]\n",
    "                    rostro = cv2.resize(rostro,(150,150), interpolation=cv2.INTER_CUBIC)\n",
    "                    cv2.imwrite(personPath + '/rotro_{}.jpg'.format(count),rostro)\n",
    "                    cv2.imshow('rostro',rostro) # Mostramos el rostro guardado \n",
    "                    count = count +1 \n",
    "\n",
    "            cv2.imshow(\"frame\", frame) # Muestra lo que esta detectando \n",
    "            if k == 27 or count >= 300: # Si tecleamos la tecla esc se cancela o si llega a 300\n",
    "                break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenarMRF(modelName):    \n",
    "    dataPath = 'C:/Users/iamra/FacialRecognition/Rostros'# Ruta donde tomara los rostros\n",
    "    peopleList = os.listdir(dataPath) # Tomamos los nombres de las personas\n",
    "    print('Lista de personas: ', peopleList)\n",
    "    labels = [] # Donde se van a guardar las etiquetas de los rostros\n",
    "    facesData = [] # Donde se van a almacecenaar todos los rostros\n",
    "    label = 0\n",
    "    for nameDir in peopleList: # Vamos a recorrer las carpetas de las caras de las personas\n",
    "        personPath = dataPath + '/' + nameDir # Escribimos la ruta de la carpeta de la persona\n",
    "        print('Leyendo las imágenes')\n",
    "        for fileName in os.listdir(personPath): # Vamos a leer todas las imagenes de las personas\n",
    "            print('Rostros: ', nameDir + '/' + fileName + '  Etiqueta:',label)\n",
    "            labels.append(label) \n",
    "            facesData.append(cv2.imread(personPath+'/'+fileName,0)) # El cero se coloca para decir que se lea en blanco y negro\n",
    "        label = label + 1 # Se suma en 1 a etiqueta cuando ya termino con una carpeta\n",
    "\n",
    "    #face_recognizer = cv2.face.EigenFaceRecognizer_create()\n",
    "    #face_recognizer = cv2.face.FisherFaceRecognizer_create()\n",
    "    face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "    print(\"Entrenando...\")\n",
    "    face_recognizer.train(facesData, np.array(labels)) # Entrena el algoritmo\n",
    "    # Almacenando el modelo obtenido\n",
    "    #face_recognizer.write('modeloEigenFace.xml')\n",
    "    #face_recognizer.write('modeloFisherFace_UyM.xml') # Escribe el algoritmo en un archivo \n",
    "    face_recognizer.write(modelName + '.xml')\n",
    "    print(\"Modelo almacenado...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9733d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenarMRE(epocas, modelName):\n",
    "    #Inicializamos dos generadores de datos de imágenes y realizamos una normalización\n",
    "    train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "    validation_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocesamos las imagenes de entrenamiento\n",
    "    directory - ruta del directorio que tiene las imagenes de entrenamiento\n",
    "    target_size - Se especifica el tamaño al que deben redimensionarse todas las imagenes\n",
    "    batch_size - Se especifica el tamaño del lote de imagenes que se cargará a la vez durante el entrenamiento\n",
    "    color_mode - Se especifica el modo de color de las imágenes cargadas en este caso se convertiran a escala de grises\n",
    "    class_mode - Especifica el modo de clasificación de las imagenes cargadas, en este caso \"categorical\" para \n",
    "    indicar que las imégenes están etiquetadas por categorías y se espera que el modelo realice una clasificación de varias clases.\n",
    "    \"\"\"\n",
    "    train_generator = train_data_gen.flow_from_directory(\n",
    "        'C:/Users/iamra/FER+/fer2013plus/fer2013/train',\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=\"categorical\"\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess las imagenes de prueba\n",
    "    \"\"\" \n",
    "    validation_generator = validation_data_gen.flow_from_directory(\n",
    "        'C:/Users/iamra/FER+/fer2013plus/fer2013/test',\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=\"categorical\"\n",
    "    )\n",
    "\n",
    "    #Creamos la estructura del modelo de red neuronal y se configuran las capas\n",
    "\n",
    "    emotion_model = Sequential() #Se crea un objeto de modelo secuencial \n",
    "\n",
    "    emotion_model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48,48,1))) #En esta capa se extraen características de las imágenes de entrada\n",
    "    emotion_model.add(Conv2D(64, kernel_size=(3,3), activation='relu')) #Extrae caracteristicas más complejas de las características extraídas por la capa anterior\n",
    "    emotion_model.add(MaxPooling2D(pool_size=(2,2))) #Se reduce la dimensionalidad de las características y ayuda a capturar patrones invarientes a escala\n",
    "    emotion_model.add(Dropout(0.25)) #Se agrega una capa para regularizar el modelo, evitando el sobreajuste, esta capa desactiva aleatoriamente el 25% de las neuronas\n",
    "    #Para evitar la dependencia de ciertas caracteristicas\n",
    "\n",
    "    emotion_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "    emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    emotion_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "    emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    emotion_model.add(Dropout(0.25))\n",
    "\n",
    "    emotion_model.add(Flatten()) #Se agrega una capa de aplanamiento para convertir el tensor de caracteristicas en un vector unidimensional, preparandolo para la capa completamente conectada\n",
    "    emotion_model.add(Dense(1024, activation='relu'))\n",
    "    emotion_model.add(Dropout(0.5))\n",
    "    emotion_model.add(Dense(5, activation='softmax')) #Se agrega la capa de salida con 4 neuronas y función de activación softmax, esta capa realiza la clasificación en 4 emociones diferentes\n",
    "\n",
    "    emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "    #Se compila el modelo con la función de pérdida de entropía cruzada categórica, el optimizador Adam con una taza de aprendizaaje de 0.00001 y un \n",
    "    #decaimiento de 1e-6 y se especifica que se desea medir la precisión durante el entrenamiento\n",
    "\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "\n",
    "    #Se entrena el modelo de la red neuronal\n",
    "\n",
    "    \"\"\"\n",
    "    Se utiliza fit_generator para entrenar el modelo utilizando generadores de datos\n",
    "    steps_per_epoch= total de imagenes de entrenamiento / batch_size,\n",
    "    epochs - cantidad de épocas que el modelo debe entrenar\n",
    "    validation_data - se pasa el generador de datos de validación \n",
    "    validation_steps= total de imagenes de validación / batch_size\n",
    "    \"\"\"\n",
    "    emotion_model_info = emotion_model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epocas,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=6824 // 64\n",
    "    )\n",
    "\n",
    "\n",
    "    #Guardamos la estructura del modelo en formato JSON\n",
    "    model_json = emotion_model.to_json()\n",
    "    with open(modelName + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    #Guardamos los pesos entrenados del modelo \n",
    "\n",
    "    emotion_model.save_weights(modelName + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd425c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llamarModelos(modelRE, modelRF):\n",
    "    # Cargar la configuración del modelo desde el archivo JSON\n",
    "    with open('C:/Users/iamra/EmotionDetection/'+ modelRE +'.json', 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "\n",
    "    # Crear el modelo a partir de la configuración cargada\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    # Cargar los pesos del modelo entrenado\n",
    "    loaded_model.load_weights('C:/Users/iamra/EmotionDetection/'+ modelRE +'.h5')\n",
    "\n",
    "    # Compilar el modelo cargado\n",
    "    loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    Datos = []\n",
    "    Nombres = []\n",
    "    Cargos = []\n",
    "    Edades = []\n",
    "    entrar_salir = [False,False]\n",
    "\n",
    "    datosPath = 'C:/Users/iamra/FacialRecognition/Datos'\n",
    "    peopleList = os.listdir(datosPath)\n",
    "    for person in peopleList: \n",
    "        filePathPerson = datosPath + '/' + person\n",
    "        with open(filePathPerson, \"r\") as file:\n",
    "            Datos.append(file.readlines())\n",
    "\n",
    "\n",
    "    for Dato in Datos:\n",
    "        aux = Dato[0].split()\n",
    "        Nombres.append(aux[1])\n",
    "        aux = Dato[2].split()\n",
    "        Cargos.append(aux[1])\n",
    "        aux = Dato[4].split()\n",
    "        Edades.append(aux[1])\n",
    "\n",
    "    #face_recognizer = cv2.face.EigenFaceRecognizer_create()\n",
    "    #face_recognizer = cv2.face.FisherFaceRecognizer_create()\n",
    "    face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "    # Leyendo el modelo\n",
    "    #face_recognizer.read('modeloEigenFace.xml')\n",
    "    #face_recognizer.read('modeloFisherFace_UyM.xml')\n",
    "    face_recognizer.read('C:/Users/iamra/FacialRecognition/'+ modelRF +'.xml')\n",
    "\n",
    "    model = loaded_model\n",
    "    #emotion_dict = {0: 'Enojado', 1:'Disgustado', 2: 'Miedo', 3:'Feliz', 4:'Neutro', 5:'Triste', 6: 'Sorprendido'}\n",
    "    emotion_dict = {0: 'Enojado',1:'Feliz', 2:'Neutro', 3:'Trizte', 4:'Sorprendido'}\n",
    "\n",
    "    # inicializar el clasificador frontal de Haar para la detección de rostros\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "    mp_face_detection = mp.solutions.face_detection # Es el detector de rostros\n",
    "    mp_drawing = mp.solutions.drawing_utils # Esto es para colocar el recuadro y los puntos\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "    # Crear una ventana con el nombre \"Ventana\"\n",
    "    cv2.namedWindow(\"Reconocimiento F&E\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # Cambiar el tamaño de la ventana para ajustarse a la pantalla completa\n",
    "    cv2.setWindowProperty(\"Reconocimiento F&E\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=False,\n",
    "        max_num_faces=2,\n",
    "        min_detection_confidence=0.5) as face_mesh:\n",
    "        with mp_face_detection.FaceDetection(  # Gestor de contexto\n",
    "            min_detection_confidence=0.6) as face_detection:\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read() # Comenzamos a leer el video\n",
    "                if ret == False: break # Verificamos si se esta leyendo de forma correcta\n",
    "                frame = imutils.resize(frame, width=640)\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                aux_frame = frame.copy()\n",
    "                height, width, _ = frame.shape # Obtenemos las dimensipones de la imagen\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # cv2 lo lee como BGR y lo convertimos a RGB       \n",
    "                results = face_detection.process(frame_rgb) # Obtenemos las cordenadas de la cara y de los puntos de interes\n",
    "                malla = face_mesh.process(frame_rgb)\n",
    "                k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\n",
    "                if results.detections is not None: # Si detecta un rostro entra aqui \n",
    "                    if malla.multi_face_landmarks is not None:\n",
    "                        for face_landmarks in malla.multi_face_landmarks:\n",
    "                            x_values = [lm.x for lm in face_landmarks.landmark]\n",
    "                            y_values = [lm.y for lm in face_landmarks.landmark]\n",
    "                            x_min, x_max = min(x_values), max(x_values)\n",
    "                            y_min, y_max = min(y_values), max(y_values)\n",
    "\n",
    "                            x = int(x_min * width)\n",
    "                            y = int(y_min * height)\n",
    "                            w = int((x_max - x_min) * width)\n",
    "                            h = int((y_max - y_min) * height)\n",
    "\n",
    "                            max_face_size = 0.8\n",
    "                            if w * h > max_face_size * width * height:\n",
    "                                continue\n",
    "\n",
    "\n",
    "                            # Recortamos el rostro y reajustamos el tamaño   \n",
    "                            rostro = aux_frame[y:y+h,x:x+w]\n",
    "                            if rostro.shape[0] != 0 and rostro.shape[1] != 0:\n",
    "                                face_gray = cv2.cvtColor(rostro, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                            try:\n",
    "                                face_gray = cv2.resize(face_gray, (48,48))\n",
    "                                face_gray = face_gray / 255.0\n",
    "                                face_gray = np.expand_dims(face_gray, axis=-1)\n",
    "                                predictions = model.predict(np.array([face_gray]))\n",
    "                                emotion = emotion_dict[np.argmax(predictions)]\n",
    "                                rostro = cv2.resize(rostro,(150,150), interpolation=cv2.INTER_CUBIC)\n",
    "                                rostro = cv2.cvtColor(rostro, cv2.COLOR_BGR2GRAY)\n",
    "                                label = face_recognizer.predict(rostro)\n",
    "                                cv2.putText(frame,'{}'.format(label),(x,y-5),1,1.3,(255,255,0),1,cv2.LINE_AA)\n",
    "                                coord_x_text = x-w-45\n",
    "                                print(label)\n",
    "\n",
    "                                if label[1] < 95:\n",
    "                                    cv2.putText(frame,'Nombre:{}'.format(Nombres[label[0]]),(coord_x_text,y-65), cv2.LINE_AA, 0.9, (0,255,0), 2)\n",
    "                                    cv2.putText(frame,'Cargo:{}'.format(Cargos[label[0]]),(coord_x_text,y-30), cv2.LINE_AA, 0.9,(0,255,0), 2)\n",
    "                                    cv2.putText(frame,'Edad:{}'.format(Edades[label[0]]),(coord_x_text,y+5), cv2.LINE_AA, 0.9,(0, 255, 0), 2)\n",
    "                                    cv2.putText(frame, 'Estado:{}'.format(emotion), (coord_x_text,y+35), cv2.LINE_AA, 0.9, (0, 255, 0), 2)\n",
    "                                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)   \n",
    "                                    mp_drawing.draw_landmarks(frame, face_landmarks,\n",
    "                                        mp_face_mesh.FACEMESH_CONTOURS , #FACEMESH_TESSELATION FACEMESH_CONTOURS  FACE_CONNECTIONS\n",
    "                                        mp_drawing.DrawingSpec(color=(0,255, 0), thickness=1, circle_radius=1),\n",
    "                                        mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1))\n",
    "\n",
    "                                else:\n",
    "                                    cv2.putText(frame,'Desconocido',(coord_x_text,y-20),cv2.LINE_AA, 0.9, (0,0,255), 2)\n",
    "                                    cv2.putText(frame, 'Estado:{}'.format(emotion), (coord_x_text, y +10), cv2.LINE_AA, 0.9, (0, 0, 255), 2)\n",
    "                                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 3)\n",
    "                                    mp_drawing.draw_landmarks(frame, face_landmarks,\n",
    "                                        mp_face_mesh.FACEMESH_CONTOURS , #FACEMESH_TESSELATION FACEMESH_CONTOURS  FACE_CONNECTIONS\n",
    "                                        mp_drawing.DrawingSpec(color=(0,0, 255), thickness=1, circle_radius=1),\n",
    "                                        mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1))\n",
    "\n",
    "                            except:\n",
    "                                continue\n",
    "                cv2.imshow(\"Reconocimiento F&E\", frame) # Muestra lo que esta detectando \n",
    "                if k == 27: break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88760f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 222ms/step\n",
      "(0, 76.30595724282047)\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "(0, 78.11974995745966)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "(0, 83.73955520022074)\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "(0, 83.17720635163931)\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "(0, 83.42859639852607)\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "(0, 82.24478145083255)\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "(0, 81.30017344452695)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "(0, 81.75505141856053)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "(0, 83.72204334264643)\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "(0, 81.47278944370467)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "(0, 82.31862596163948)\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "(0, 81.21202940545929)\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "(0, 78.63008858711937)\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "(0, 76.42454241077853)\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "(0, 78.02012454712568)\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "(0, 76.78606357694579)\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "(0, 78.25726104173738)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "(0, 78.97735630229045)\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "(0, 78.59900939034303)\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "(0, 78.8731886193214)\n"
     ]
    }
   ],
   "source": [
    "llamarModelos('emotion_model5Emotions', 'modeloLBPHFace_OTG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c8aa98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizarDatos(personName, newCargo, newEdad):\n",
    "    # Ruta al directorio donde se encuentran los archivos de texto\n",
    "    dataPath_txt = 'C:/Users/iamra/FacialRecognition/Datos'  # Cambiar en la otra computadora\n",
    "\n",
    "    # Ruta al archivo de la persona\n",
    "    filePath = os.path.join(dataPath_txt, f'{personName}.txt')\n",
    "\n",
    "    # Comprobar si el archivo existe\n",
    "    if os.path.exists(filePath):\n",
    "        # Actualizar los datos en el archivo\n",
    "        with open(filePath, \"r\") as file:\n",
    "            datos = file.readlines()\n",
    "\n",
    "        datos_actualizados = []\n",
    "        for linea in datos:\n",
    "            if linea.startswith(\"Nombre:\"):\n",
    "                datos_actualizados.append(f\"Nombre: {personName}\\n\")\n",
    "            elif linea.startswith(\"Cargo\"):\n",
    "                datos_actualizados.append(f\"Cargo: {newCargo}\\n\")\n",
    "            elif linea.startswith(\"Edad\"):\n",
    "                datos_actualizados.append(f\"Edad: {newEdad}\\n\")\n",
    "            else:\n",
    "                datos_actualizados.append(linea)\n",
    "\n",
    "        with open(filePath, \"w\") as file:\n",
    "            file.writelines(datos_actualizados)\n",
    "            print(f\"Datos actualizados para {personName}.\")\n",
    "\n",
    "        # Actualizar las listas Nombres, Cargos y Edades\n",
    "        Nombres = []\n",
    "        Cargos = []\n",
    "        Edades = []\n",
    "\n",
    "        peopleList = os.listdir(dataPath_txt)\n",
    "        for person in peopleList:\n",
    "            filePathPerson = os.path.join(dataPath_txt, person)\n",
    "            with open(filePathPerson, \"r\") as file:\n",
    "                datos = file.readlines()\n",
    "\n",
    "            for linea in datos:\n",
    "                if linea.startswith(\"Nombre:\"):\n",
    "                    nombre = linea.strip().split(\":\")[1].strip()\n",
    "                elif linea.startswith(\"Cargo:\"):\n",
    "                    cargo = linea.strip().split(\":\")[1].strip()\n",
    "                elif linea.startswith(\"Edad:\"):\n",
    "                    edad = linea.strip().split(\":\")[1].strip()\n",
    "\n",
    "            Nombres.append(nombre)\n",
    "            Cargos.append(cargo)\n",
    "            Edades.append(edad)\n",
    "\n",
    "        # Imprimir los datos actualizados\n",
    "        print(Nombres)\n",
    "        print(Cargos)\n",
    "        print(Edades)\n",
    "    else:\n",
    "        print(f\"El archivo para {personName} no existe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d62b37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminarRegistro(personName):\n",
    "    # Rutas de la carpeta, video y archivo de texto\n",
    "    carpeta_path = f'C:/Users/iamra/FacialRecognition/Rostros/{personName}'\n",
    "    video_path = f'C:/Users/iamra/FacialRecognition/Videos/{personName}.mp4'\n",
    "    txt_path = f'C:/Users/iamra/FacialRecognition/Datos/{personName}.txt'\n",
    "\n",
    "    # Eliminar la carpeta\n",
    "    if os.path.exists(carpeta_path):\n",
    "        os.rmdir(carpeta_path)\n",
    "        print(f\"Carpeta {personName} eliminada.\")\n",
    "    else:\n",
    "        print(f\"Carpeta {personName} no encontrada.\")\n",
    "\n",
    "    # Eliminar el video\n",
    "    if os.path.exists(video_path):\n",
    "        os.remove(video_path)\n",
    "        print(f\"Video {personName}.mp4 eliminado.\")\n",
    "    else:\n",
    "        print(f\"Video {personName}.mp4 no encontrado.\")\n",
    "\n",
    "    # Eliminar el archivo de texto\n",
    "    if os.path.exists(txt_path):\n",
    "        os.remove(txt_path)\n",
    "        print(f\"Archivo {personName}.txt eliminado.\")\n",
    "    else:\n",
    "        print(f\"Archivo {personName}.txt no encontrado.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
