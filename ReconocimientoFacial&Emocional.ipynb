{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4d8614",
   "metadata": {},
   "source": [
    "<h1>Sistema de Reconocimiento Facial Emocional</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b88e2",
   "metadata": {},
   "source": [
    "## 1. Importamos las librerias y paquetes requeridos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae30a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias a utilizar\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import imutils\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential #Se utiliza para crear modelos de redes neuronales secuenciales (Es aquel en el que las capas se apilan una encima de la otra)\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam #Se utiliza para ajustar los pesos y los sesgos de las redes neuronales durante el entrenamiento\n",
    "from keras.preprocessing.image import ImageDataGenerator #Permite generar imagenes aumentadas en tiempo real durante el entrenamiento, ayuda a aumentar el tamaño y la diversidad del conjunto de datos lo que mejora la capacidad de generalización del modelo\n",
    "\"\"\"\n",
    "Capas de las redes neuronales:\n",
    "Conv2D - Se realiza la convulsión en imagenes de entrada para extraer características/patrones importantes\n",
    "MaxPooling2D - Reduce la dimensionalidad y extrae características dominantes\n",
    "Dense - Es una capa completamente conectada, cada entrada se conecta con todos los nodos de salida\n",
    "Dropout - Esta capa se utiliza para regularizar el modelo y evitar el sobreajuste, apaga neuronas aleatoriamente durante el entrenamiento para evitar la dependencia excesiva\n",
    "Flatten - Aplana los datos multidimensionales en una matriz unidimensional antes de pasarlo a las capas completamente conectadas\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e5ab56",
   "metadata": {},
   "source": [
    "## 2. Método para Almacenar Rostro del usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183a266",
   "metadata": {},
   "source": [
    "La función almacenarRostro(personName) tiene como objetivo capturar un video utilizando la cámara del dispositivo y almacenarlo en formato mp4. A continuación, se proporciona una descripción de lo que hace la función y los elementos que requiere:\n",
    "\n",
    "personName: Parámetro que indica el nombre del archivo de video a almacenar. Este nombre se utilizará para generar la ruta completa del archivo de video.\n",
    "La función comienza pausando la ejecución durante 3 segundos utilizando time.sleep(3). Esto puede ser útil para permitir que el dispositivo se prepare adecuadamente antes de comenzar a grabar el video.\n",
    "\n",
    "A continuación, se establece la ruta de almacenamiento del video utilizando dataPath_mp4 y el nombre de la persona proporcionado. El video se almacenará en la carpeta especificada.\n",
    "\n",
    "Luego, se verifica si ya existe un video con el mismo nombre en la ruta especificada. Si no existe, se procede a capturar el video utilizando la cámara del dispositivo.\n",
    "\n",
    "Dentro del bucle while, se obtienen los frames de video de la cámara utilizando captura.read(). Luego, se aplica un efecto espejo a la imagen utilizando cv2.flip(imagen, 1). Si se detecta correctamente un frame de video, se muestra en una ventana titulada \"video\" utilizando cv2.imshow('video', imagen). Además, el frame de video se escribe en el archivo de salida utilizando salida.write(imagen).\n",
    "\n",
    "Si se presiona la tecla 's', el bucle se interrumpe y se finaliza la captura de video. El archivo de video se guarda en la ubicación especificada.\n",
    "\n",
    "Si ya existe un video con el mismo nombre en la ruta especificada, se muestra un mensaje indicando que ya existe un video con ese nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e076d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def almacenarRostro(personName):\n",
    "    time.sleep(3)\n",
    "    dataPath_mp4 = 'C:/Users/iamra/FacialRecognition/Videos' #Cambia a la ruta donde hayas almacenado Data\n",
    "    videoPath = dataPath_mp4 + '/' + personName + '.mp4' #Ruta completa\n",
    "\n",
    "    if not os.path.exists(videoPath): # Si no existe el video con ese nombre lo crea\n",
    "        captura = cv2.VideoCapture(0) # Definioms que usaremos la camara\n",
    "        salida = cv2.VideoWriter(videoPath,cv2.VideoWriter_fourcc(*'XVID'),20.0,(640,480)) # Cambiar con la otra camara\n",
    "        while True:\n",
    "            ret, imagen = captura.read() # Comienza a grabar\n",
    "            imagen = cv2.flip(imagen, 1) # Aplicamos efecto espejo\n",
    "            if ret == True: # Si detecta video\n",
    "                cv2.imshow('video', imagen) # Mostramos la imagen\n",
    "                salida.write(imagen) # Comenzamos a guardar la imagen\n",
    "                if cv2.waitKey(1) & 0xFF == ord('s'): # Si se presiona la tecla s, paramos de grabar y se guarda el video\n",
    "                      break\n",
    "            else: break\n",
    "        captura.release()\n",
    "        salida.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    else: \n",
    "        print(\"Ya existe un video con ese nombre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb797962",
   "metadata": {},
   "source": [
    "## 3. Método para almacenar Datos del usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b403eb",
   "metadata": {},
   "source": [
    "La función almacenarDatos(personName, cargo, edad) tiene como objetivo almacenar información personal en un archivo de texto. A continuación, se proporciona una descripción de lo que hace la función y los elementos que requiere:\n",
    "\n",
    "personName: Parámetro que indica el nombre de la persona cuyos datos se van a almacenar.\n",
    "cargo: Parámetro que indica el cargo de la persona.\n",
    "edad: Parámetro que indica la edad de la persona.\n",
    "La función comienza estableciendo la ruta de almacenamiento del archivo de texto utilizando dataPath_txt y el nombre de la persona proporcionado. El archivo de texto se almacenará en la carpeta especificada.\n",
    "\n",
    "A continuación, se verifica si ya existe un archivo con el mismo nombre en la ruta especificada. Si no existe, se procede a crear el archivo de texto y escribir los datos en él.\n",
    "\n",
    "Dentro del bloque with open(filePath, \"w\") as file:, se abre el archivo en modo de escritura (\"w\"). Luego, se escriben los datos de la persona en líneas separadas utilizando file.write(). Los datos que se escriben son el nombre, el cargo y la edad de la persona.\n",
    "\n",
    "Si ya existe un archivo con el mismo nombre en la ruta especificada, se muestra un mensaje indicando que ya existe un archivo con esos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7c4c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def almacenarDatos(personName, cargo, edad):\n",
    "    dataPath_txt = 'C:/Users/iamra/FacialRecognition/Datos' # Cambiar en la otra computadora\n",
    "    filePath = dataPath_txt + '/' + personName + '.txt'\n",
    "\n",
    "    if not os.path.exists(filePath):\n",
    "        with open(filePath, \"w\") as file:\n",
    "            file.write(\"Nombre: \" + personName + os.linesep)\n",
    "            file.write(\"Cargo: \" + cargo +  os.linesep) \n",
    "            file.write(\"Edad: \" + edad)\n",
    "\n",
    "    else:\n",
    "        print(\"Ya existe un archivo con estos datos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c6d53",
   "metadata": {},
   "source": [
    "## 4. Método para generar un informe con los usuarios registrados "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f683799f",
   "metadata": {},
   "source": [
    "La función generarInformes() tiene como objetivo recopilar y mostrar los datos almacenados en archivos de texto previamente guardados. A continuación, se proporciona una descripción de lo que hace la función:\n",
    "\n",
    "La función comienza estableciendo la ruta de acceso a la carpeta que contiene los archivos de texto utilizando dataPath_txt.\n",
    "\n",
    "A continuación, se inicializan las listas Datos, Nombres, Cargos y Edades, que se utilizarán para almacenar la información extraída de los archivos de texto.\n",
    "\n",
    "Se obtiene la lista de archivos presentes en la carpeta utilizando os.listdir(dataPath_txt). Luego, se itera sobre cada archivo de la lista en el bucle for person in peopleList.\n",
    "\n",
    "Dentro del bucle, se construye la ruta completa del archivo de texto utilizando filePathPerson. Luego, se abre el archivo en modo de lectura (\"r\") utilizando with open(filePathPerson, \"r\") as file:. A continuación, se lee el contenido del archivo utilizando file.readlines() y se agrega a la lista Datos.\n",
    "\n",
    "Posteriormente, se itera sobre cada elemento de la lista Datos en el bucle for Dato in Datos. Dentro del bucle, se divide cada línea en palabras utilizando split() y se extrae el nombre, cargo y edad correspondientes. Estos valores se agregan a las listas Nombres, Cargos y Edades respectivamente.\n",
    "\n",
    "Finalmente, se imprimen en la consola las listas Nombres, Cargos y Edades, lo que muestra los datos recopilados de los archivos de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96255957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generarInformes():\n",
    "    dataPath_txt = 'C:/Users/iamra/FacialRecognition/Datos'\n",
    "    Datos = []\n",
    "    Nombres = []\n",
    "    Cargos = []\n",
    "    Edades = []\n",
    "\n",
    "    peopleList = os.listdir(dataPath_txt)\n",
    "    for person in peopleList: \n",
    "        filePathPerson = dataPath_txt + '/' + person\n",
    "        with open(filePathPerson, \"r\") as file:\n",
    "            Datos.append(file.readlines())\n",
    "\n",
    "\n",
    "    for Dato in Datos:\n",
    "        aux = Dato[0].split()\n",
    "        Nombres.append(aux[1])\n",
    "        aux = Dato[2].split()\n",
    "        Cargos.append(aux[1])\n",
    "        aux = Dato[4].split()\n",
    "        Edades.append(aux[1])\n",
    "\n",
    "    print(Nombres)\n",
    "    print(Cargos)\n",
    "    print(Edades)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44120bc",
   "metadata": {},
   "source": [
    "## 5. Método para extraer los rostros "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dfc8f5",
   "metadata": {},
   "source": [
    "La función extraerRostro(personName) tiene como objetivo extraer y almacenar rostros de un video específico en una carpeta designada. A continuación, se proporciona una descripción de lo que hace la función:\n",
    "\n",
    "La función comienza estableciendo la ruta de almacenamiento de los rostros en dataPath, y crea una subcarpeta para la persona específica utilizando personPath.\n",
    "\n",
    "Se verifica si la carpeta de la persona ya existe en la ruta especificada. Si no existe, se crea la carpeta utilizando os.makedirs(personPath). Además, se imprime en la consola un mensaje indicando la creación de la carpeta.\n",
    "\n",
    "A continuación, se utiliza el detector de rostros de MediaPipe (mp_face_detection) y la utilidad de dibujo (mp_drawing) para detectar y dibujar recuadros alrededor de los rostros en el video.\n",
    "\n",
    "Se abre el video utilizando cv2.VideoCapture, especificando la ruta del video correspondiente al nombre de la persona proporcionado.\n",
    "\n",
    "Se inicializa una variable de contador (count) que llevará la cuenta de las fotos (rostros) que se han tomado.\n",
    "\n",
    "Dentro del bucle while, se lee cada frame del video utilizando cap.read(). Si no se puede leer correctamente, se sale del bucle.\n",
    "\n",
    "Se redimensiona el frame a un ancho de 640 píxeles utilizando imutils.resize(frame, width=640). Luego, se obtienen las dimensiones del frame utilizando frame.shape.\n",
    "\n",
    "Se copia el frame en una variable auxiliar (aux_frame) para trabajar con ella. Se convierte el frame de BGR a RGB utilizando cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).\n",
    "\n",
    "Se procesa el frame con el detector de rostros utilizando face_detection.process(frame_rgb). Si se detecta uno o más rostros en el frame, se entra en el bucle for y se realizan las siguientes acciones:\n",
    "\n",
    "Se dibuja un recuadro alrededor de cada rostro y se resaltan los puntos de interés utilizando mp_drawing.draw_detection(frame, detection, ...).\n",
    "Se obtienen las coordenadas del rostro en el frame utilizando detection.location_data.relative_bounding_box.\n",
    "Se recorta y ajusta el tamaño del rostro utilizando las coordenadas obtenidas.\n",
    "Se guarda el rostro en la carpeta correspondiente utilizando cv2.imwrite(personPath + '/rostro_{}.jpg'.format(count), rostro).\n",
    "Se muestra el rostro guardado en una ventana separada utilizando cv2.imshow('rostro', rostro).\n",
    "Se incrementa el contador de fotos (count).\n",
    "Además, se muestra el frame en una ventana utilizando cv2.imshow(\"frame\", frame).\n",
    "\n",
    "El bucle continúa hasta que se presione la tecla 'Esc' o hasta que se alcance un límite de 300 rostros guardados.\n",
    "\n",
    "Finalmente, se libera el video y se cierran las ventanas utilizando cap.release() y cv2.destroyAllWindows() respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbc2b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraerRostro(personName):\n",
    "    dataPath = 'C:/Users/iamra/FacialRecognition/Rostros'# Lugar donde se van a almacenar los rostros\n",
    "    personPath = dataPath + '/' + personName \n",
    "\n",
    "    if not os.path.exists(personPath): # Verifica si existe la carpete de la persona si no crea una\n",
    "        print('Carpeta creada: ',personPath)\n",
    "        os.makedirs(personPath)\n",
    "\n",
    "    mp_face_detection = mp.solutions.face_detection # Es el detector de rostros\n",
    "    mp_drawing = mp.solutions.drawing_utils # Esto es para colocar el recuadro y los puntos\n",
    "    cap = cv2.VideoCapture('C:/Users/iamra/FacialRecognition/Videos/' + personName + '.mp4')\n",
    "\n",
    "    count = 0 # Esta variable va a llevar la cuenta de las fotos que se han tomado\n",
    "\n",
    "    with mp_face_detection.FaceDetection(  # Gestor de contexto\n",
    "        min_detection_confidence=0.6) as face_detection:\n",
    "        while True:\n",
    "            ret, frame = cap.read() # Comenzamos a leer el video\n",
    "            if ret == False: # Verificamos si se esta leyendo de forma correcta\n",
    "                break\n",
    "            frame = imutils.resize(frame, width=640)\n",
    "            height, width, _ = frame.shape # Obtenemos las dimensipones de la imagen\n",
    "            aux_frame = frame.copy() # Copiamos el frame \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # cv2 lo lee como BGR y lo convertimos a RGB       \n",
    "            results = face_detection.process(frame_rgb) # Obtenemos las cordenadas de la cara y de los puntos de interes\n",
    "            k = cv2.waitKey(1) & 0xFF\n",
    "            if results.detections is not None: # Si detecta un rostro entra aqui \n",
    "                for detection in results.detections: # Se usa un for en caso de detectar mas de un rostro\n",
    "                    mp_drawing.draw_detection(frame, detection,\n",
    "                        mp_drawing.DrawingSpec(color=(0, 255, 255), circle_radius=2),\n",
    "                        mp_drawing.DrawingSpec(color=(255, 0, 255))) # Dibuja en un recuadro un rostro y con circulos los puntos de interes\n",
    "\n",
    "                    # Obtenemos la posicion de la cara\n",
    "                    x = int(detection.location_data.relative_bounding_box.xmin * width)\n",
    "                    y = int(detection.location_data.relative_bounding_box.ymin * height)\n",
    "                    w = int(detection.location_data.relative_bounding_box.width * width)\n",
    "                    h = int(detection.location_data.relative_bounding_box.height * height)\n",
    "\n",
    "                    # Recortamos el rostro y reajustamos el tamaño\n",
    "                    rostro = aux_frame[y:y+h,x:x+w]\n",
    "                    rostro = cv2.resize(rostro,(150,150), interpolation=cv2.INTER_CUBIC)\n",
    "                    cv2.imwrite(personPath + '/rotro_{}.jpg'.format(count),rostro)\n",
    "                    cv2.imshow('rostro',rostro) # Mostramos el rostro guardado \n",
    "                    count = count +1 \n",
    "\n",
    "            cv2.imshow(\"frame\", frame) # Muestra lo que esta detectando \n",
    "            if k == 27 or count >= 300: # Si tecleamos la tecla esc se cancela o si llega a 300\n",
    "                break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11854d1f",
   "metadata": {},
   "source": [
    "## 6. Método para entrenar el Modelo de Reconocimiento Facial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc395f3",
   "metadata": {},
   "source": [
    "La función entrenarMRF(modelName) tiene como objetivo entrenar un modelo de reconocimiento facial utilizando el algoritmo de reconocimiento LBPH (Local Binary Patterns Histograms). A continuación, se proporciona una descripción de lo que hace la función:\n",
    "\n",
    "La función comienza estableciendo la ruta de acceso a la carpeta que contiene los rostros de entrenamiento en dataPath. Luego, se obtiene una lista de los nombres de las personas presentes en la carpeta utilizando os.listdir(dataPath).\n",
    "\n",
    "Se inicializan las listas labels y facesData, que se utilizarán para almacenar las etiquetas y los datos de los rostros respectivamente.\n",
    "\n",
    "Se inicializa la variable label en 0, que se utilizará para asignar etiquetas a los rostros.\n",
    "\n",
    "Se itera sobre cada nombre de directorio en la lista peopleList utilizando el bucle for nameDir in peopleList. Dentro del bucle, se construye la ruta completa de la carpeta de la persona utilizando personPath.\n",
    "\n",
    "Se itera sobre cada nombre de archivo en la carpeta de la persona utilizando os.listdir(personPath). Dentro del bucle, se agrega la etiqueta correspondiente a la lista labels, y se lee la imagen del rostro en escala de grises utilizando cv2.imread(personPath+'/'+fileName,0). La imagen del rostro se agrega a la lista facesData.\n",
    "\n",
    "Después de leer todas las imágenes de una persona, se incrementa la etiqueta en 1 utilizando label = label + 1.\n",
    "\n",
    "A continuación, se inicializa el reconocedor de rostros utilizando el algoritmo LBPH mediante cv2.face.LBPHFaceRecognizer_create().\n",
    "\n",
    "Se realiza el entrenamiento del reconocedor de rostros utilizando face_recognizer.train(facesData, np.array(labels)). Los datos de los rostros y las etiquetas se pasan como argumentos.\n",
    "\n",
    "Finalmente, se almacena el modelo entrenado en un archivo utilizando face_recognizer.write(modelName + '.xml'), donde modelName es el nombre del archivo especificado como parámetro en la función. Se muestra un mensaje indicando que el modelo ha sido almacenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenarMRF(modelName):    \n",
    "    dataPath = 'C:/Users/iamra/FacialRecognition/Rostros'# Ruta donde tomara los rostros\n",
    "    peopleList = os.listdir(dataPath) # Tomamos los nombres de las personas\n",
    "    print('Lista de personas: ', peopleList)\n",
    "    labels = [] # Donde se van a guardar las etiquetas de los rostros\n",
    "    facesData = [] # Donde se van a almacecenaar todos los rostros\n",
    "    label = 0\n",
    "    for nameDir in peopleList: # Vamos a recorrer las carpetas de las caras de las personas\n",
    "        personPath = dataPath + '/' + nameDir # Escribimos la ruta de la carpeta de la persona\n",
    "        print('Leyendo las imágenes')\n",
    "        for fileName in os.listdir(personPath): # Vamos a leer todas las imagenes de las personas\n",
    "            print('Rostros: ', nameDir + '/' + fileName + '  Etiqueta:',label)\n",
    "            labels.append(label) \n",
    "            facesData.append(cv2.imread(personPath+'/'+fileName,0)) # El cero se coloca para decir que se lea en blanco y negro\n",
    "        label = label + 1 # Se suma en 1 a etiqueta cuando ya termino con una carpeta\n",
    "\n",
    "    #face_recognizer = cv2.face.EigenFaceRecognizer_create()\n",
    "    #face_recognizer = cv2.face.FisherFaceRecognizer_create()\n",
    "    face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "    print(\"Entrenando...\")\n",
    "    face_recognizer.train(facesData, np.array(labels)) # Entrena el algoritmo\n",
    "    # Almacenando el modelo obtenido\n",
    "    #face_recognizer.write('modeloEigenFace.xml')\n",
    "    #face_recognizer.write('modeloFisherFace_UyM.xml') # Escribe el algoritmo en un archivo \n",
    "    face_recognizer.write(modelName + '.xml')\n",
    "    print(\"Modelo almacenado...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3a475",
   "metadata": {},
   "source": [
    "## 7. Método para entrenar el Modelo de Reconocimiento Emocional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b85ef7",
   "metadata": {},
   "source": [
    "La función entrenarMRE(epocas, modelName) tiene como objetivo entrenar un modelo de red neuronal convolucional (CNN) para el reconocimiento de emociones. A continuación, se proporciona una descripción de lo que hace la función:\n",
    "\n",
    "La función utiliza el generador de datos ImageDataGenerator de Keras para preprocesar las imágenes de entrenamiento y validación. Las imágenes se normalizan dividiendo los valores de píxeles por 255.\n",
    "\n",
    "Se configuran dos generadores de datos: train_generator para las imágenes de entrenamiento y validation_generator para las imágenes de validación. Se especifica la ruta del directorio que contiene las imágenes de entrenamiento y validación, el tamaño objetivo de las imágenes, el tamaño del lote, el modo de color (escala de grises) y el modo de clasificación (categórico).\n",
    "\n",
    "A continuación, se crea la estructura del modelo de red neuronal utilizando el objeto Sequential. Se añaden capas convolucionales (Conv2D), capas de agrupación (MaxPooling2D), capas de regularización (Dropout) y capas completamente conectadas (Dense). La última capa de salida tiene una activación softmax para clasificar las emociones en 5 categorías diferentes.\n",
    "\n",
    "Se compila el modelo utilizando la función de pérdida categorical_crossentropy, el optimizador Adam con una tasa de aprendizaje de 0.0001 y un decaimiento de 1e-6, y se especifica que se desea medir la precisión durante el entrenamiento.\n",
    "\n",
    "Se calcula el número de pasos por época (steps_per_epoch) dividiendo el número total de imágenes de entrenamiento por el tamaño del lote.\n",
    "\n",
    "A continuación, se entrena el modelo utilizando el método fit y el generador de datos de entrenamiento y validación. Se especifica el número de épocas, el generador de datos de validación y los pasos de validación.\n",
    "\n",
    "Después de completar el entrenamiento, se guarda la estructura del modelo en formato JSON utilizando to_json() y se guarda en un archivo .json. También se guardan los pesos entrenados del modelo en un archivo .h5 utilizando save_weights()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9733d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenarMRE(epocas, modelName):\n",
    "    #Inicializamos dos generadores de datos de imágenes y realizamos una normalización\n",
    "    train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "    validation_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocesamos las imagenes de entrenamiento\n",
    "    directory - ruta del directorio que tiene las imagenes de entrenamiento\n",
    "    target_size - Se especifica el tamaño al que deben redimensionarse todas las imagenes\n",
    "    batch_size - Se especifica el tamaño del lote de imagenes que se cargará a la vez durante el entrenamiento\n",
    "    color_mode - Se especifica el modo de color de las imágenes cargadas en este caso se convertiran a escala de grises\n",
    "    class_mode - Especifica el modo de clasificación de las imagenes cargadas, en este caso \"categorical\" para \n",
    "    indicar que las imégenes están etiquetadas por categorías y se espera que el modelo realice una clasificación de varias clases.\n",
    "    \"\"\"\n",
    "    train_generator = train_data_gen.flow_from_directory(\n",
    "        'C:/Users/iamra/FER+/fer2013plus/fer2013/train',\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=\"categorical\"\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess las imagenes de prueba\n",
    "    \"\"\" \n",
    "    validation_generator = validation_data_gen.flow_from_directory(\n",
    "        'C:/Users/iamra/FER+/fer2013plus/fer2013/test',\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=\"categorical\"\n",
    "    )\n",
    "\n",
    "    #Creamos la estructura del modelo de red neuronal y se configuran las capas\n",
    "\n",
    "    emotion_model = Sequential() #Se crea un objeto de modelo secuencial \n",
    "\n",
    "    emotion_model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48,48,1))) #En esta capa se extraen características de las imágenes de entrada\n",
    "    emotion_model.add(Conv2D(64, kernel_size=(3,3), activation='relu')) #Extrae caracteristicas más complejas de las características extraídas por la capa anterior\n",
    "    emotion_model.add(MaxPooling2D(pool_size=(2,2))) #Se reduce la dimensionalidad de las características y ayuda a capturar patrones invarientes a escala\n",
    "    emotion_model.add(Dropout(0.25)) #Se agrega una capa para regularizar el modelo, evitando el sobreajuste, esta capa desactiva aleatoriamente el 25% de las neuronas\n",
    "    #Para evitar la dependencia de ciertas caracteristicas\n",
    "\n",
    "    emotion_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "    emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    emotion_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "    emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    emotion_model.add(Dropout(0.25))\n",
    "\n",
    "    emotion_model.add(Flatten()) #Se agrega una capa de aplanamiento para convertir el tensor de caracteristicas en un vector unidimensional, preparandolo para la capa completamente conectada\n",
    "    emotion_model.add(Dense(1024, activation='relu'))\n",
    "    emotion_model.add(Dropout(0.5))\n",
    "    emotion_model.add(Dense(5, activation='softmax')) #Se agrega la capa de salida con 4 neuronas y función de activación softmax, esta capa realiza la clasificación en 4 emociones diferentes\n",
    "\n",
    "    emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "    #Se compila el modelo con la función de pérdida de entropía cruzada categórica, el optimizador Adam con una taza de aprendizaaje de 0.00001 y un \n",
    "    #decaimiento de 1e-6 y se especifica que se desea medir la precisión durante el entrenamiento\n",
    "\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "\n",
    "    #Se entrena el modelo de la red neuronal\n",
    "\n",
    "    \"\"\"\n",
    "    Se utiliza fit_generator para entrenar el modelo utilizando generadores de datos\n",
    "    steps_per_epoch= total de imagenes de entrenamiento / batch_size,\n",
    "    epochs - cantidad de épocas que el modelo debe entrenar\n",
    "    validation_data - se pasa el generador de datos de validación \n",
    "    validation_steps= total de imagenes de validación / batch_size\n",
    "    \"\"\"\n",
    "    emotion_model_info = emotion_model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epocas,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=6824 // 64\n",
    "    )\n",
    "\n",
    "\n",
    "    #Guardamos la estructura del modelo en formato JSON\n",
    "    model_json = emotion_model.to_json()\n",
    "    with open(modelName + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    #Guardamos los pesos entrenados del modelo \n",
    "\n",
    "    emotion_model.save_weights(modelName + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c389361",
   "metadata": {},
   "source": [
    "## 8. Método para ejecutar el sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bcc5e2",
   "metadata": {},
   "source": [
    "La función llamarModelos(modelRE, modelRF) tiene como objetivo cargar y utilizar los modelos previamente entrenados para el reconocimiento de emociones y reconocimiento facial respectivamente. A continuación, se proporciona una descripción de lo que hace la función:\n",
    "\n",
    "La función comienza cargando la configuración del modelo de reconocimiento de emociones desde un archivo JSON. Luego, crea el modelo utilizando la configuración cargada y carga los pesos del modelo entrenado desde un archivo .h5. El modelo cargado se compila con una función de pérdida categorical_crossentropy, un optimizador adam y se especifica que se desea medir la precisión.\n",
    "\n",
    "A continuación, se carga la información de los datos de las personas desde los archivos de texto en la ruta especificada en datosPath. Se recopilan los nombres, cargos y edades de las personas en listas separadas.\n",
    "\n",
    "Se carga el modelo de reconocimiento facial utilizando la función read() de OpenCV. Se especifica la ruta del archivo XML que contiene la información del modelo.\n",
    "\n",
    "Se inicializa la captura de video utilizando la cámara. Se utiliza el clasificador frontal de Haar (haarcascade_frontalface_default.xml) para detectar rostros en el video.\n",
    "\n",
    "Dentro del bucle principal, se procesa cada frame del video. Se redimensiona el frame y se convierte a RGB. Se utilizan los detectores de rostros y de malla facial de MediaPipe para obtener las coordenadas de los rostros y los puntos de interés en el rostro.\n",
    "\n",
    "Si se detecta un rostro en el frame, se extrae el rostro y se realiza el reconocimiento de emociones utilizando el modelo de emociones cargado. También se realiza el reconocimiento facial utilizando el modelo de reconocimiento facial cargado. Los resultados se muestran en el frame con etiquetas de nombre, cargo, edad y estado emocional. Se dibuja un rectángulo alrededor del rostro y se dibujan los puntos de la malla facial si están disponibles.\n",
    "\n",
    "Finalmente, se muestra el frame procesado en una ventana. El bucle continúa hasta que se presione la tecla 'Esc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd425c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llamarModelos(modelRE, modelRF):\n",
    "    # Cargar la configuración del modelo desde el archivo JSON\n",
    "    with open('C:/Users/iamra/EmotionDetection/'+ modelRE +'.json', 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "\n",
    "    # Crear el modelo a partir de la configuración cargada\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    # Cargar los pesos del modelo entrenado\n",
    "    loaded_model.load_weights('C:/Users/iamra/EmotionDetection/'+ modelRE +'.h5')\n",
    "\n",
    "    # Compilar el modelo cargado\n",
    "    loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    Datos = []\n",
    "    Nombres = []\n",
    "    Cargos = []\n",
    "    Edades = []\n",
    "    entrar_salir = [False,False]\n",
    "\n",
    "    datosPath = 'C:/Users/iamra/FacialRecognition/Datos'\n",
    "    peopleList = os.listdir(datosPath)\n",
    "    for person in peopleList: \n",
    "        filePathPerson = datosPath + '/' + person\n",
    "        with open(filePathPerson, \"r\") as file:\n",
    "            Datos.append(file.readlines())\n",
    "\n",
    "\n",
    "    for Dato in Datos:\n",
    "        aux = Dato[0].split()\n",
    "        Nombres.append(aux[1])\n",
    "        aux = Dato[2].split()\n",
    "        Cargos.append(aux[1])\n",
    "        aux = Dato[4].split()\n",
    "        Edades.append(aux[1])\n",
    "\n",
    "    #face_recognizer = cv2.face.EigenFaceRecognizer_create()\n",
    "    #face_recognizer = cv2.face.FisherFaceRecognizer_create()\n",
    "    face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "    # Leyendo el modelo\n",
    "    #face_recognizer.read('modeloEigenFace.xml')\n",
    "    #face_recognizer.read('modeloFisherFace_UyM.xml')\n",
    "    face_recognizer.read('C:/Users/iamra/FacialRecognition/'+ modelRF +'.xml')\n",
    "\n",
    "    model = loaded_model\n",
    "    #emotion_dict = {0: 'Enojado', 1:'Disgustado', 2: 'Miedo', 3:'Feliz', 4:'Neutro', 5:'Triste', 6: 'Sorprendido'}\n",
    "    emotion_dict = {0: 'Enojado',1:'Feliz', 2:'Neutro', 3:'Trizte', 4:'Sorprendido'}\n",
    "\n",
    "    # inicializar el clasificador frontal de Haar para la detección de rostros\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "    mp_face_detection = mp.solutions.face_detection # Es el detector de rostros\n",
    "    mp_drawing = mp.solutions.drawing_utils # Esto es para colocar el recuadro y los puntos\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "    # Crear una ventana con el nombre \"Ventana\"\n",
    "    cv2.namedWindow(\"Reconocimiento F&E\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # Cambiar el tamaño de la ventana para ajustarse a la pantalla completa\n",
    "    cv2.setWindowProperty(\"Reconocimiento F&E\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=False,\n",
    "        max_num_faces=2,\n",
    "        min_detection_confidence=0.5) as face_mesh:\n",
    "        with mp_face_detection.FaceDetection(  # Gestor de contexto\n",
    "            min_detection_confidence=0.6) as face_detection:\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read() # Comenzamos a leer el video\n",
    "                if ret == False: break # Verificamos si se esta leyendo de forma correcta\n",
    "                frame = imutils.resize(frame, width=640)\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                aux_frame = frame.copy()\n",
    "                height, width, _ = frame.shape # Obtenemos las dimensipones de la imagen\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # cv2 lo lee como BGR y lo convertimos a RGB       \n",
    "                results = face_detection.process(frame_rgb) # Obtenemos las cordenadas de la cara y de los puntos de interes\n",
    "                malla = face_mesh.process(frame_rgb)\n",
    "                k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\n",
    "                if results.detections is not None: # Si detecta un rostro entra aqui \n",
    "                    if malla.multi_face_landmarks is not None:\n",
    "                        for face_landmarks in malla.multi_face_landmarks:\n",
    "                            x_values = [lm.x for lm in face_landmarks.landmark]\n",
    "                            y_values = [lm.y for lm in face_landmarks.landmark]\n",
    "                            x_min, x_max = min(x_values), max(x_values)\n",
    "                            y_min, y_max = min(y_values), max(y_values)\n",
    "\n",
    "                            x = int(x_min * width)\n",
    "                            y = int(y_min * height)\n",
    "                            w = int((x_max - x_min) * width)\n",
    "                            h = int((y_max - y_min) * height)\n",
    "\n",
    "                            max_face_size = 0.8\n",
    "                            if w * h > max_face_size * width * height:\n",
    "                                continue\n",
    "\n",
    "\n",
    "                            # Recortamos el rostro y reajustamos el tamaño   \n",
    "                            rostro = aux_frame[y:y+h,x:x+w]\n",
    "                            if rostro.shape[0] != 0 and rostro.shape[1] != 0:\n",
    "                                face_gray = cv2.cvtColor(rostro, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                            try:\n",
    "                                face_gray = cv2.resize(face_gray, (48,48))\n",
    "                                face_gray = face_gray / 255.0\n",
    "                                face_gray = np.expand_dims(face_gray, axis=-1)\n",
    "                                predictions = model.predict(np.array([face_gray]))\n",
    "                                emotion = emotion_dict[np.argmax(predictions)]\n",
    "                                rostro = cv2.resize(rostro,(150,150), interpolation=cv2.INTER_CUBIC)\n",
    "                                rostro = cv2.cvtColor(rostro, cv2.COLOR_BGR2GRAY)\n",
    "                                label = face_recognizer.predict(rostro)\n",
    "                                cv2.putText(frame,'{}'.format(label),(x,y-5),1,1.3,(255,255,0),1,cv2.LINE_AA)\n",
    "                                coord_x_text = x-w-45\n",
    "                                print(label)\n",
    "\n",
    "                                if label[1] < 95:\n",
    "                                    cv2.putText(frame,'Nombre:{}'.format(Nombres[label[0]]),(coord_x_text,y-65), cv2.LINE_AA, 0.9, (0,255,0), 2)\n",
    "                                    cv2.putText(frame,'Cargo:{}'.format(Cargos[label[0]]),(coord_x_text,y-30), cv2.LINE_AA, 0.9,(0,255,0), 2)\n",
    "                                    cv2.putText(frame,'Edad:{}'.format(Edades[label[0]]),(coord_x_text,y+5), cv2.LINE_AA, 0.9,(0, 255, 0), 2)\n",
    "                                    cv2.putText(frame, 'Estado:{}'.format(emotion), (coord_x_text,y+35), cv2.LINE_AA, 0.9, (0, 255, 0), 2)\n",
    "                                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)   \n",
    "                                    mp_drawing.draw_landmarks(frame, face_landmarks,\n",
    "                                        mp_face_mesh.FACEMESH_CONTOURS , #FACEMESH_TESSELATION FACEMESH_CONTOURS  FACE_CONNECTIONS\n",
    "                                        mp_drawing.DrawingSpec(color=(0,255, 0), thickness=1, circle_radius=1),\n",
    "                                        mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1))\n",
    "\n",
    "                                else:\n",
    "                                    cv2.putText(frame,'Desconocido',(coord_x_text,y-20),cv2.LINE_AA, 0.9, (0,0,255), 2)\n",
    "                                    cv2.putText(frame, 'Estado:{}'.format(emotion), (coord_x_text, y +10), cv2.LINE_AA, 0.9, (0, 0, 255), 2)\n",
    "                                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 3)\n",
    "                                    mp_drawing.draw_landmarks(frame, face_landmarks,\n",
    "                                        mp_face_mesh.FACEMESH_CONTOURS , #FACEMESH_TESSELATION FACEMESH_CONTOURS  FACE_CONNECTIONS\n",
    "                                        mp_drawing.DrawingSpec(color=(0,0, 255), thickness=1, circle_radius=1),\n",
    "                                        mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1))\n",
    "\n",
    "                            except:\n",
    "                                continue\n",
    "                cv2.imshow(\"Reconocimiento F&E\", frame) # Muestra lo que esta detectando \n",
    "                if k == 27: break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241c662",
   "metadata": {},
   "source": [
    "## 9. Método para actualizar los registros del usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b7207",
   "metadata": {},
   "source": [
    "La función actualizarDatos(personName, newCargo, newEdad) tiene como objetivo actualizar los datos de una persona previamente registrada. A continuación, se proporciona una descripción de lo que hace la función:\n",
    "\n",
    "La función comienza construyendo la ruta al archivo de texto correspondiente a la persona especificada utilizando el nombre de la persona proporcionado en el parámetro personName. Luego, se verifica si el archivo existe.\n",
    "\n",
    "Si el archivo existe, se lee su contenido y se guarda en la variable datos. A continuación, se recorre cada línea del archivo y se comprueba si corresponde a la línea que contiene el nombre, el cargo o la edad. En caso afirmativo, se actualiza el valor correspondiente con los nuevos datos proporcionados en los parámetros newCargo y newEdad. Las demás líneas se mantienen sin cambios.\n",
    "\n",
    "Después de actualizar los datos en el archivo, se sobrescribe el archivo con los nuevos datos actualizados. Luego, se actualizan las listas Nombres, Cargos y Edades con los datos actualizados de todos los archivos de texto en el directorio.\n",
    "\n",
    "Finalmente, se imprime la lista actualizada de nombres, cargos y edades. Si el archivo no existe, se muestra un mensaje de que el archivo correspondiente a la persona no existe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c8aa98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizarDatos(personName, newCargo, newEdad):\n",
    "    # Ruta al directorio donde se encuentran los archivos de texto\n",
    "    dataPath_txt = 'C:/Users/iamra/FacialRecognition/Datos'  # Cambiar en la otra computadora\n",
    "\n",
    "    # Ruta al archivo de la persona\n",
    "    filePath = os.path.join(dataPath_txt, f'{personName}.txt')\n",
    "\n",
    "    # Comprobar si el archivo existe\n",
    "    if os.path.exists(filePath):\n",
    "        # Actualizar los datos en el archivo\n",
    "        with open(filePath, \"r\") as file:\n",
    "            datos = file.readlines()\n",
    "\n",
    "        datos_actualizados = []\n",
    "        for linea in datos:\n",
    "            if linea.startswith(\"Nombre:\"):\n",
    "                datos_actualizados.append(f\"Nombre: {personName}\\n\")\n",
    "            elif linea.startswith(\"Cargo\"):\n",
    "                datos_actualizados.append(f\"Cargo: {newCargo}\\n\")\n",
    "            elif linea.startswith(\"Edad\"):\n",
    "                datos_actualizados.append(f\"Edad: {newEdad}\\n\")\n",
    "            else:\n",
    "                datos_actualizados.append(linea)\n",
    "\n",
    "        with open(filePath, \"w\") as file:\n",
    "            file.writelines(datos_actualizados)\n",
    "            print(f\"Datos actualizados para {personName}.\")\n",
    "\n",
    "        # Actualizar las listas Nombres, Cargos y Edades\n",
    "        Nombres = []\n",
    "        Cargos = []\n",
    "        Edades = []\n",
    "\n",
    "        peopleList = os.listdir(dataPath_txt)\n",
    "        for person in peopleList:\n",
    "            filePathPerson = os.path.join(dataPath_txt, person)\n",
    "            with open(filePathPerson, \"r\") as file:\n",
    "                datos = file.readlines()\n",
    "\n",
    "            for linea in datos:\n",
    "                if linea.startswith(\"Nombre:\"):\n",
    "                    nombre = linea.strip().split(\":\")[1].strip()\n",
    "                elif linea.startswith(\"Cargo:\"):\n",
    "                    cargo = linea.strip().split(\":\")[1].strip()\n",
    "                elif linea.startswith(\"Edad:\"):\n",
    "                    edad = linea.strip().split(\":\")[1].strip()\n",
    "\n",
    "            Nombres.append(nombre)\n",
    "            Cargos.append(cargo)\n",
    "            Edades.append(edad)\n",
    "\n",
    "        # Imprimir los datos actualizados\n",
    "        print(Nombres)\n",
    "        print(Cargos)\n",
    "        print(Edades)\n",
    "    else:\n",
    "        print(f\"El archivo para {personName} no existe.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07d74a",
   "metadata": {},
   "source": [
    "## 10. Método para eliminar registros de usuarios existentes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4708dc",
   "metadata": {},
   "source": [
    "La función eliminarRegistro(personName) tiene como objetivo eliminar el registro de una persona en el sistema de reconocimiento facial. A continuación, se proporciona una descripción de lo que hace la función:\n",
    "\n",
    "La función comienza construyendo las rutas a la carpeta que contiene los rostros, el video y el archivo de texto correspondientes a la persona especificada utilizando el nombre de la persona proporcionado en el parámetro personName.\n",
    "\n",
    "A continuación, se verifica si la carpeta existe en la ruta especificada. Si la carpeta existe, se elimina utilizando os.rmdir(carpeta_path). Se muestra un mensaje indicando que la carpeta ha sido eliminada. Si la carpeta no existe, se muestra un mensaje indicando que la carpeta no fue encontrada.\n",
    "\n",
    "Luego, se verifica si el video existe en la ruta especificada. Si el video existe, se elimina utilizando os.remove(video_path). Se muestra un mensaje indicando que el video ha sido eliminado. Si el video no existe, se muestra un mensaje indicando que el video no fue encontrado.\n",
    "\n",
    "Finalmente, se verifica si el archivo de texto existe en la ruta especificada. Si el archivo de texto existe, se elimina utilizando os.remove(txt_path). Se muestra un mensaje indicando que el archivo de texto ha sido eliminado. Si el archivo de texto no existe, se muestra un mensaje indicando que el archivo de texto no fue encontrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d62b37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminarRegistro(personName):\n",
    "    # Rutas de la carpeta, video y archivo de texto\n",
    "    carpeta_path = f'C:/Users/iamra/FacialRecognition/Rostros/{personName}'\n",
    "    video_path = f'C:/Users/iamra/FacialRecognition/Videos/{personName}.mp4'\n",
    "    txt_path = f'C:/Users/iamra/FacialRecognition/Datos/{personName}.txt'\n",
    "\n",
    "    # Eliminar la carpeta\n",
    "    if os.path.exists(carpeta_path):\n",
    "        os.rmdir(carpeta_path)\n",
    "        print(f\"Carpeta {personName} eliminada.\")\n",
    "    else:\n",
    "        print(f\"Carpeta {personName} no encontrada.\")\n",
    "\n",
    "    # Eliminar el video\n",
    "    if os.path.exists(video_path):\n",
    "        os.remove(video_path)\n",
    "        print(f\"Video {personName}.mp4 eliminado.\")\n",
    "    else:\n",
    "        print(f\"Video {personName}.mp4 no encontrado.\")\n",
    "\n",
    "    # Eliminar el archivo de texto\n",
    "    if os.path.exists(txt_path):\n",
    "        os.remove(txt_path)\n",
    "        print(f\"Archivo {personName}.txt eliminado.\")\n",
    "    else:\n",
    "        print(f\"Archivo {personName}.txt no encontrado.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
